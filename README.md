# [NeurIPS 2024] Low Precision Local Training is Enough for Federated Learning

## Introduction
We propose an efficient FL paradigm, where the local  models in the clients are trained with  low-precision operations and communicated  with the server in low precision format, while only the model aggregation in the server is performed with high-precision computation.  Our paradigm can be integrated with existing FL algorithms flexibly. Notably, the models trained by our method with the precision as low as 8 bits are  comparable  to those from the  full precision training. Please see the paper for details.

## Begin

## Cite

If you find our paper useful for your research and applications, please kindly cite using this BibTeX:

```latex
@inproceedings{
lilow,
title={Low Precision Local Training is Enough for Federated Learning},
author={Zhiwei, Li and Yiqiu, Li and Binbin, Lin and Zhongming, Jin and Weizhong, Zhang},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}
```
